# All-Replicate Content Engine (MVP)

Local dev quickstart

1. Start infra (Postgres + MinIO):
   - `docker compose -f infra/docker-compose.yml up -d`
   - Create bucket `gpt5video` in MinIO console (http://localhost:9001)
2. API
   - Copy `apps/api/ENV_EXAMPLE.md` to `.env` and set `REPLICATE_API_TOKEN`
   - `npm run -w apps/api dev`
3. GUI
   - Copy `apps/gui/ENV_LOCAL_EXAMPLE.md` to `.env.local`
   - `npm run -w apps/gui dev`

Week 1 demo

- Brand: paste JSON and submit
- Scenes Plan: author one spec or an array; validate and submit
- Scenes Render: submit a valid spec; see prediction id, model version, seed, duration; artifacts will appear on Renders
- Video Assemble: submit a manifest with `audio.mode: none`

Notes on models

- ideogram-character inputs must include `prompt`, `character_reference_image`, with optional `aspect_ratio` and `rendering_speed`
- imagen-4 inputs must include `prompt` with optional `aspect_ratio` and `negative_prompt`

## GPT-5 Video — Monorepo

Operator-driven pipeline to go from Hooks → Scenes → Video using Replicate models, with strict JSON schemas, signed uploads, and an SSE-powered GUI.

### Repository layout

- `apps/api`: Express.js API with AJV validation, SSE, Replicate calls, signed upload helpers
- `apps/gui`: Next.js GUI with Tailwind and React Query
- `packages/schemas`: JSON Schemas (Draft 2020-12) + generated types (WIP)
- `packages/shared`: Shared runtime validators (AJV) and types
- `packages/clients/*`: External clients (e.g., storage, replicate)
- `infra/`: Docker Compose for local Postgres + MinIO
- `prd.md`: Product Requirements Document (source of truth for scope)
- `to-do.md`: Living build plan and weekly checklist (single source of truth for work status)

### Prerequisites

- Node.js 20+
- npm 9+
- Docker Desktop (for local Postgres + MinIO)

### First-time setup

1. Install dependencies

```bash
npm install
```

2. Build packages (ensures linked workspaces with `dist/` are ready)

```bash
npm run build
```

3. Start local infra (only needed if using MinIO + Postgres locally)

```bash
docker compose -f infra/docker-compose.yml up -d
```

- MinIO Console: http://localhost:9001 (user: `minioadmin`, pass: `minioadmin`)
- Create a bucket named `gpt5video` if testing against MinIO

### Environment configuration

API environment file lives at `apps/api/.env`. Copy from the example and fill secrets.

```bash
cp apps/api/ENV_EXAMPLE.md /tmp/ENV_API_EXAMPLE.md  # open and copy values into apps/api/.env
```

Minimal values (choose one storage provider):

- MinIO (local)

```ini
PORT=4000
LOG_LEVEL=info
POSTGRES_URL=postgres://postgres:postgres@localhost:5432/gpt5video

S3_ENDPOINT=http://localhost:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_REGION=us-east-1
S3_BUCKET=gpt5video
```

- Cloudflare R2 (recommended for dev/prod) — use account-level endpoint and region `auto`

```ini
PORT=4000
LOG_LEVEL=info
POSTGRES_URL=postgres://postgres:postgres@localhost:5432/gpt5video

S3_ENDPOINT=https://<account_id>.r2.cloudflarestorage.com
S3_REGION=auto
S3_ACCESS_KEY=<access_key_id>
S3_SECRET_KEY=<secret_access_key>
S3_BUCKET=<bucket_name>
```

GUI environment file lives at `apps/gui/.env.local`:

```ini
NEXT_PUBLIC_API_BASE=http://localhost:4000
```

### Running the services

- API (in one terminal):

```bash
npm run dev -w @gpt5video/api
```

- GUI (in another terminal):

```bash
npm run dev -w @gpt5video/gui
```

Optional: if you edit code in `packages/clients/storage`, start a local watcher:

```bash
npm run dev -w @gpt5video/storage
```

Endpoints:

- API base: `http://localhost:4000`
- GUI: `http://localhost:3000`

### Quick verification

1. Health check

```bash
curl -s http://localhost:4000/health
```

2. Generate presigned PUT for an object (API signs against your configured S3/R2)

```bash
curl -s -X POST http://localhost:4000/uploads/sign \
  -H "Content-Type: application/json" \
  --data-raw '{"key":"dev/test.txt","content_type":"text/plain"}'
```

Copy the `url` from the response and upload a small file:

```bash
echo "hello world" > /tmp/dev-test.txt
curl -i -X PUT "<paste presigned url>" \
  -H "Content-Type: text/plain" \
  --data-binary @/tmp/dev-test.txt
```

3. Verify with a presigned GET (generated by API):

```bash
curl -s "http://localhost:4000/uploads/debug-get?key=dev/test.txt"
```

Use the returned URL to fetch the object:

```bash
curl -i "<paste presigned get url>"
```

### Storage notes (Cloudflare R2)

- Use the account-level endpoint (no bucket in the endpoint URL): `https://<account_id>.r2.cloudflarestorage.com`
- Region must be `auto`
- Bucket name is provided via the `Bucket` parameter in API calls; object key is path-style (`/<bucket>/<key>`)
- For presigned URLs, the request you send must match what was signed: method, `Content-Type`, and headers
- Checksums: the storage client is configured to avoid extra checksum headers that R2 may not accept in presigned URLs

### Troubleshooting

- `404 NoSuchKey` on presigned GET: the object was not uploaded. Re-check that the PUT used the exact presigned URL and matching `Content-Type`.
- Signature mismatch or 403: ensure your PUT uses the same headers that were included in signing (especially `Content-Type`).
- Cannot connect to MinIO: confirm Docker Desktop is running and `docker compose -f infra/docker-compose.yml up -d` succeeded. Visit http://localhost:9001.
- API won’t start due to schemas: run `npm run build` to ensure `packages/*` are compiled, then restart the API.

### Useful scripts

- Build all workspaces: `npm run build`
- Dev all (runs available `dev` scripts): `npm run dev`
- Lint all: `npm run lint`
- Typecheck all: `npm run typecheck`

### Working agreements

- Align changes with `prd.md`
- Keep `to-do.md` updated as the single source of truth for plan and status
- Schema-first development: update `packages/schemas`, generate types, validate at runtime in API
